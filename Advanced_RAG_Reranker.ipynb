{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Advanced RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Retrieval-Augmented Generation (RAG) Overview\n",
    "\n",
    "Naive RAG involves three primary phases:\n",
    "\n",
    "1. **Indexing**: \n",
    "   - Prepares the document collection for retrieval by cleaning and extracting relevant information from each document. This phase involves parsind and preprocessing documents, chunking the parsed documents, using an embedding model to generate vectors out of the chunks, and storing them into a vector database.\n",
    "   \n",
    "2. **Retrieval**: \n",
    "   - Converts the user query into a vector representation using the embedding model.\n",
    "   - Compares the vectorized query with the vectors stored in the vector database to retrieve the most relevant chunks. \n",
    "\n",
    "3. **Generation**: \n",
    "   - Augments the user query with the retrieved chunks into a single prompt.\n",
    "   - Generates an answer to the user query using a language model based on this combined information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RAG Pitfalls\n",
    "\n",
    "Naive RAG presents two main types of challenges:\n",
    "1. **Retrieval challenges**: selection of irrelevant chunks to query, missing crucial information.\n",
    "2. **Generation challenges**: the LLM struggles with hallucination and presents issues with relevance in its output.\n",
    "\n",
    "These challenges occur mainly due to:\n",
    "\n",
    "1. **Limited contextual understanding**:\n",
    "   - Because naive RAG focus on **keyword matching** or basic semantic search, it retrieves irrelevant or partially relevant documents to the query. For example, a query like \"*...the impact of climate change on polar bears*\" would retrieve documents related with *climate change* and *polars bears* but not documents that talk about both *climate change* and *polars bears*.\n",
    "\n",
    "2. **Inconsistent Relevance and Quality of Retrieved Documents**  \n",
    "   - Naive RAG may struggle with effective document ranking, which can result in irrelevant or low-quality inputs being fed into the model.\n",
    "\n",
    "3. **Poor Integration Between Retrieval and Generation**  \n",
    "   - In Naive RAG, the retriever and generator operate independently, often leading to a lack of coordination between retrieval results and generation context.\n",
    "\n",
    "4. **Inefficient Handling of Large-Scale Data**  \n",
    "   - Standard retrieval methods can be inefficient at scale, making it challenging for Naive RAG to manage large datasets. This can cause delays in finding relevant documents or result in critical information being missed due to ineffective indexing.\n",
    "\n",
    "5. **Lack of Robustness and Adaptability**  \n",
    "   - Naive RAG lacks mechanisms to handle ambiguous or complex queries effectively. When queries contain multiple or nuanced questions, Naive RAG struggles to adapt, often failing to provide comprehensive answers due to its limited flexibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced RAG Techniques\n",
    "Advanced RAG techniques introduces improvement to overcome the limitations of Naive RAG, focusing on enhancing the retrieval quality.\n",
    "\n",
    "1. Pre-retrieval\n",
    "    - It improves the indexing structure and user's query, by organizing the indexes better and adding extra information. \n",
    "\n",
    "2. Pos-retrieval\n",
    "    - We combine the data obtained during the pre-retrieval phase with the original query. This could involve reraking to highlight the most important content. \n",
    "\n",
    "### RAG with Reranking for Enhanced Document Retrieval\n",
    "\n",
    "In the naive RAG (Retrieval-Augmented Generation) approach, we retrieve the top *n* most relevant chunks for a given query. This retrieval process involves embedding the query and comparing it to the embedded chunks stored in a vector database. The similarity is typically calculated based on the cosine similarity between the query embedding and the stored chunk embeddings. The top *n* most similar chunks are then returned. However, this approach has limitations in terms of relevance, as it relies solely on simple metrics such as the cosine similarity for ranking.\n",
    "\n",
    "A **reranker** generally outperforms a retriever because it incorporates a more sophisticated ranking process. Specifically, it often involves a transformer-based inference step, which is significantly more complex and contextually aware than simple cosine similarity. By adding a reranker, we can initially retrieve a broader set of candidate chunks using the retriever, and then pass only the most relevant chunks to our language model for processing. The reranker refines the order of these retrieved chunks, ensuring that the most pertinent items appear at the top.\n",
    "\n",
    "For example, if we have a query and 100 chunks in our vector database, the retriever first generates an initial relevance ranking based on cosine similarity and returns, for example, the 25 most similar chunks to the query. Then, the reranker further evaluates the retrieved chunks, reordering them based on deeper contextual analysis. \n",
    "\n",
    "While reranking can significantly improve the relevance of retrieved documents, it comes with certain drawbacks:\n",
    "\n",
    "1. **Empirical Evaluation:** It can be challenging to evaluate the effectiveness of reranking in consistently retrieving better documents.\n",
    "2. **Processing Time:** The reranker generally requires more computational resources and time, as transformer-based similarity calculations are more intensive than cosine similarity calculations.\n",
    "\n",
    "By balancing the retriever and reranker components, we can achieve a more precise and contextually relevant retrieval pipeline, optimizing the relevance of results without overwhelming computational resources.\n",
    "\n",
    "##  Reranking Implementation\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
